<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>My First Post2</title><link rel=stylesheet href=https://bilosikia.github.io/css/colors-dark.min.91962b8a364d509ff3d76bb0de34f672d0f09e1485bda7795118d692ecb4e0ea.css></head><body><header id=header><h1><a href=https://bilosikia.github.io>bilosikia's site</a></h1><p></p></header><div id=page><div id=sidebar><nav></nav></div><div id=content><article class=post><h1><a href=https://bilosikia.github.io/posts/my-first-post2/>My First Post2</a></h1><div class=post-content><p>We decouple the flow of data from the flow of control to
use the networkefficiently. While control flows from the
client to the primary and then to all secondaries, data is
pushed linearly along a carefully picked chain of chunkservers
in a pipelined fashion. Our goals are to fully utilize each
machine’s networkbandwidth, avoid networkbottlenecks
and high-latency links, and minimize the latency to push
through all the data.
To fully utilize each machine’s networkbandwidth, the
data is pushed linearly along a chain of chunkservers rather
than distributed in some other topology (e.g., tree). Thus,
each machine’s full outbound bandwidth is used to transfer the data as fast as possible rather than divided among
multiple recipients.
To avoid network bottlenecks and high-latency links (e.g.,
inter-switch links are often both) as much as possible, each
machine forwards the data to the “closest” machine in the
networktopology that has not received it. Suppose the
client is pushing data to chunkservers S1 through S4. It
sends the data to the closest chunkserver, say S1. S1 forwards it to the closest chunkserver S2 through S4 closest to
S1, say S2. Similarly, S2 forwards it to S3 or S4, whichever
is closer to S2, and so on. Our networktopology is simple
enough that “distances” can be accurately estimated from
IP addresses.
Finally, we minimize latency by pipelining the data transfer over TCP connections. Once a chunkserver receives some
data, it starts forwarding immediately. Pipelining is especially helpful to us because we use a switched networkwith
full-duplex links. Sending the data immediately does not
reduce the receive rate. Without networkcongestion, the
ideal elapsed time for transferring B bytes to R replicas is
B/T + RL where T is the networkthroughput and L is latency to transfer bytes between two machines. Our network
links are typically 100 Mbps (T), and L is far below 1 ms.
Therefore, 1 MB can ideally be distributed in about 80 ms.</p></div><p class=meta>Posted on <span class=postdate>16. April 2021</span></p></article></div><footer id=footer><p class=copyright>Powered by <a href=https://gohugo.io/>Hugo</a> and the
<a href=https://github.com/bake/solar-theme-hugo>Solar</a>-theme.</p></footer></div></body></html>